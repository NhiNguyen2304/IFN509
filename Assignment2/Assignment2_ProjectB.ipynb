{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV dataset file name\n",
    "file_name = 'D2.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please put csv file in the same folder with this jupyter notebook\n",
    "df = pd.read_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Preprocessing\n",
    "- What pre-processing was required on the dataset (D2.csv) before building the clustering model on the chosen attributes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proposed changes:\n",
    "- Readmitted is currently stored in binary format  while change and readmitted are bool. Will need to identify which format is required for use in clustering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for Q2 - 3. \n",
    "#Create dataframe of selected variables for use in clustering model creation\n",
    "df1 = df[[\"num_lab_procedures\", \"number_outpatient\", \"number_inpatient\", \"num_medications\", \"time_in_hospital\"]]\n",
    "print(df1.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualise distribution of variables to identify potential data problems. \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# Distribution of num_lab_procedures\n",
    "num_lab_procedures_dist = sns.distplot(df1['num_lab_procedures'].dropna())\n",
    "plt.show()\n",
    "# Distribution of number_outpatient\n",
    "number_outpatient_dist = sns.distplot(df1['number_outpatient'].dropna(), bins=100)\n",
    "plt.show()\n",
    "# Distribution of number_inpatient\n",
    "number_inpatient_dist = sns.distplot(df1['number_inpatient'].dropna(), bins=100)\n",
    "plt.show()\n",
    "# Distribution of num_medications\n",
    "num_medications_dist = sns.distplot(df1['num_medications'].dropna())\n",
    "plt.show()\n",
    "# Distribution of time_in_hospital\n",
    "time_in_hospital_dist = sns.distplot(df1['time_in_hospital'].dropna(), bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables are on different scales, need to standardise the scaling of variables to allow for model accuracy. \n",
    "print(df1.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling process. \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# convert df1 to matrix\n",
    "X = df1.to_numpy()\n",
    "\n",
    "# scaling\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing for Q4\n",
    "print(df['age'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping\n",
    "age_map = {'[0-10)':1, '[10-20)':2, '[20-30)':3, '[30-40)':4, '[40-50)':5, '[50-60)':6, '[60-70)':7, '[70-80)':8, '[80-90)':9, '[90-100)':10}\n",
    "#print(age_map)\n",
    "df['age'] = df['age'].map(age_map)\n",
    "print(df['age'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataframe of selected variables for use in clustering model creation\n",
    "df2 = df[[\"num_lab_procedures\", \"number_outpatient\", \"number_inpatient\", \"num_medications\", \"time_in_hospital\", \"age\"]]\n",
    "print(df2.info())\n",
    "print(\"AGE Values:\", df2['age'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import StandardScaler\n",
    "# convert df to matrix\n",
    "X2 = df2.to_numpy()\n",
    "\n",
    "# scaling\n",
    "scaler = StandardScaler()\n",
    "X2 = scaler.fit_transform(X2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2 & 3 \n",
    "## Clustering Model 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model using scaled df created above\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# random state, we will use 42 instead of 10 for a change\n",
    "rs = 42\n",
    "\n",
    "# set the random state. different random state seeds might result in \n",
    "# different centr\n",
    "model = KMeans(n_clusters=3, random_state=rs)\n",
    "model.fit(X)\n",
    "\n",
    "# sum of intra-cluster distances\n",
    "print(\"Sum of intra-cluster distance:\", model.inertia_)\n",
    "\n",
    "print(\"Centroid locations:\")\n",
    "for centroid in model.cluster_centers_:\n",
    "     print(centroid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=3, random_state=rs).fit(X)\n",
    "\n",
    "# assign cluster ID to each record in X\n",
    "# Ignore the warning, does not apply to our case here\n",
    "y = model.predict(X)\n",
    "df1['Cluster_ID'] = y\n",
    "\n",
    "# how many records are in each cluster\n",
    "print(\"Cluster membership\")\n",
    "print(df1['Cluster_ID'].value_counts())\n",
    "\n",
    "# pairplot the cluster distribution.\n",
    "cluster_g = sns.pairplot(df1, hue='Cluster_ID',diag_kind='hist')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MP Insights\n",
    "- The greater the number of medications in cluster 0, the fewer visits to inpatient or outpatient hospital\n",
    "- The number of medications for patients in cluster 2 doesn't seem to impact on their length of stay in hospital. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create plots for each variable's distribution in a cluster against the overall data set distribution. \n",
    "# prepare the column and bin size. Increase bin size to be more specific, but 20 is\n",
    "cols = [\"num_lab_procedures\", \"number_outpatient\", \"number_inpatient\", \"num_medications\", \"time_in_hospital\"]\n",
    "n_bins = 20\n",
    "\n",
    "# inspecting cluster 0 and 1\n",
    "clusters_to_inspect = [0,1,2]\n",
    "\n",
    "for cluster in clusters_to_inspect:\n",
    "    # inspecting cluster 0\n",
    "    print(\"Distribution for cluster {}\".format(cluster))\n",
    "     \n",
    "    # create subplots\n",
    "    fig, ax = plt.subplots(nrows=5)\n",
    "    ax[0].set_title(\"Cluster {}\".format(cluster))\n",
    "\n",
    "    for j, col in enumerate(cols):\n",
    "        # create the bins\n",
    "        bins = np.linspace(min(df1[col]), max(df1[col]), 30)\n",
    "        # plot distribution of the cluster using histogram\n",
    "        sns.distplot(df1[df1['Cluster_ID'] == cluster][col], bins=bins, ax=ax[j], norm_hist=True)\n",
    "        # plot the normal distribution with a black line\n",
    "        sns.distplot(df1[col], bins=bins, ax=ax[j], hist=False, color=\"k\")\n",
    "\n",
    "    #plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list to save the clusters and cost\n",
    "clusters = []\n",
    "inertia_vals = []\n",
    "\n",
    "# this whole process should take a while\n",
    "for k in range(2, 15, 2):\n",
    "    # train clustering with the specified K\n",
    "    model = KMeans(n_clusters=k, random_state=rs, n_jobs=10)\n",
    "    model.fit(X)\n",
    "    \n",
    "    # append model to cluster list\n",
    "    clusters.append(model)\n",
    "    inertia_vals.append(model.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the inertia vs K values\n",
    "plt.plot(range(2,15,2), inertia_vals, marker='*')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate silhouette scores for points k = 4, k = 6 and k = 8.\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "print(clusters[1])\n",
    "print(\"Silhouette score for k=4\", silhouette_score(X, clusters[1].predict(X)))\n",
    "\n",
    "print(clusters[2])\n",
    "print(\"Silhouette score for k=6\", silhouette_score(X, clusters[2].predict(X)))\n",
    "\n",
    "print(clusters[2])\n",
    "print(\"Silhouette score for k=8\", silhouette_score(X, clusters[3].predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MP Insights\n",
    "- it appears that k=6 is the optimal number of clusters for this model according to the silhouette score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation of K=6 clustering solution\n",
    "model = KMeans(n_clusters=6, random_state=rs)\n",
    "model.fit(X)\n",
    "\n",
    "# sum of intra-cluster distances\n",
    "print(\"Sum of intra-cluster distance:\", model.inertia_)\n",
    "\n",
    "print(\"Centroid locations:\")\n",
    "for centroid in model.cluster_centers_:\n",
    "    print(centroid)\n",
    "\n",
    "y = model.predict(X)\n",
    "df1['Cluster_ID'] = y\n",
    "\n",
    "# how many in each\n",
    "print(\"Cluster membership\")\n",
    "print(df1['Cluster_ID'].value_counts())\n",
    "\n",
    "# pairplot\n",
    "# added alpha value to assist with overlapping points\n",
    "cluster_g = sns.pairplot(df1, hue='Cluster_ID', diag_kind='hist')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "## Clustering Model 2 - includes AGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kmodes.kmodes import KModes\n",
    "from kmodes.kprototypes import KPrototypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list to save the clusters and cost\n",
    "clusters = []\n",
    "cost_vals = []\n",
    "\n",
    "# this whole process should take a while\n",
    "for k in range(2, 10, 2):\n",
    "    # train clustering with the specified K\n",
    "    model = KPrototypes(n_clusters=k, random_state=rs, n_jobs=10)\n",
    "    model.fit_predict(X2, categorical=[1])\n",
    "    \n",
    "    # append model to cluster list\n",
    "    clusters.append(model)\n",
    "    cost_vals.append(model.cost_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
