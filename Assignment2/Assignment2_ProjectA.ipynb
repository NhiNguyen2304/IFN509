{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV dataset file name\n",
    "file_name = 'D1.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please put csv file in the same folder with this jupyter notebook\n",
    "df = pd.read_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for '?' as 'Invalid/Unknown'\n",
    "for col in df.columns:\n",
    "    print(col,df[col][df[col] == 'NaN'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quantity variable should be in int64 data type. Before converting to integer, this variable is rounded to become integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Quantity'] = df['Quantity'].apply(np.ceil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Quantity'] = df['Quantity'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load formarted dataframe into dataset file\n",
    "df.to_csv(file_name)\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Perform Association mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need to add first:\n",
    "-measures of relationships between variables to identify interestingness of relationship. This will allow us to provide more info on what to do in terms of association mining. \n",
    "-data appears clean on first glance. \n",
    "-data types appear appropriate on first glance\n",
    "-is there interestingness in the correlations in specific values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remaining cells are Mel's version of using the tutorial 8 code with the assignment data. 13/9/22\n",
    "#group by account, then list all services\n",
    "transactions = df.groupby(['Customer_ID'])['SKU_Category'].apply(list)\n",
    "\n",
    "print(transactions.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to install apyori at home using terminal command pip install apyori\n",
    "from apyori import apriori\n",
    "\n",
    "#type cast the transactions from panadas into normal list format and \n",
    "#run apriori\n",
    "transaction_list = list(transactions)\n",
    "results = list(apriori(transaction_list, min_support = 0.055, min_confidence = 0.055))\n",
    "# 0.07 => loose Lift more than 1 (item with very lift is interesting)\n",
    "\n",
    "#print first 5 rules\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define new min_support and min_confidence to looking for '01F' product category\n",
    "results_3 = list(apriori(transaction_list, min_support = 0.015, min_confidence = 0.015))\n",
    "# 0.07 => loose Lift more than 1 (item with very lift is interesting)\n",
    "\n",
    "#print first 5 rules\n",
    "print(results_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_apriori_results_to_pandas_df(results):\n",
    "    rules = []\n",
    "    \n",
    "    for rule_set in results:\n",
    "        for rule in rule_set.ordered_statistics:\n",
    "            #items_base = left side of rules, items_add = right side\n",
    "            #support, confidence, lift for respective rules\n",
    "            rules.append([','.join(rule.items_base), ','.join(rule.items_add), rule_set.support, \n",
    "                          rule.confidence, rule.lift])\n",
    "        \n",
    "    #typecaset to pandas df\n",
    "    return pd.DataFrame(rules, columns = ['Left_side', 'Right_side', 'Support', 'Confidence', 'Lift'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = convert_apriori_results_to_pandas_df(results)\n",
    "\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result_df_3 = convert_apriori_results_to_pandas_df(results_3)\n",
    "\n",
    "print(result_df_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = result_df.sort_values(by='Lift', ascending = False)\n",
    "print(result_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the frequntly items \n",
    "result_df[(result_df['Lift'] > 1) &\n",
    "                   (result_df['Support'] >= 0.05) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing Sequential Rule Mining Using SPMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = df.groupby(['Customer_ID'])['SKU_Category'].apply(list)\n",
    "sequences = transactions.values.tolist()\n",
    "\n",
    "# show the first 5 sequences\n",
    "print(sequences[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import subprocess\n",
    "import re\n",
    "\n",
    "''' Uses SPMF to find association rules in supplied transactions '''\n",
    "def get_association_rules(sequences, min_sup, min_conf):\n",
    "    # step 1: create required input for SPMF\n",
    "    \n",
    "    # prepare a dict to uniquely assign each item in the transactions to an int ID\n",
    "    item_dict = defaultdict(int)\n",
    "    output_dict = defaultdict(str)\n",
    "    item_id = 1\n",
    "    \n",
    "    # write your sequences in SPMF format\n",
    "    with open('seq_rule_input.txt', 'w+') as f:\n",
    "        for sequence in sequences:\n",
    "            z = []\n",
    "            for itemset in sequence:\n",
    "                # if there are multiple items in one itemset\n",
    "                if isinstance(itemset, list):\n",
    "                    for item in itemset:\n",
    "                        if item not in item_dict:\n",
    "                            item_dict[item] = item_id\n",
    "                            item_id += 1\n",
    "\n",
    "                        z.append(item_dict[item])\n",
    "                else:\n",
    "                    if itemset not in item_dict:\n",
    "                        item_dict[itemset] = item_id\n",
    "                        output_dict[str(item_id)] = itemset\n",
    "                        item_id += 1\n",
    "                    z.append(item_dict[itemset])\n",
    "                    \n",
    "                # end of itemset\n",
    "                z.append(-1)\n",
    "            \n",
    "            # end of a sequence\n",
    "            z.append(-2)\n",
    "            f.write(' '.join([str(x) for x in z]))\n",
    "            f.write('\\n')\n",
    "    \n",
    "    # run SPMF with supplied parameters\n",
    "    supp_param = '{}%'.format(int(min_sup * 100))\n",
    "    conf_param = '{}%'.format(int(min_conf * 100))\n",
    "    subprocess.call(['java', '-jar', 'spmf.jar', 'run', 'RuleGrowth', 'seq_rule_input.txt', 'seq_rule_output.txt', '0.55%', '0.55%'], shell=True)\n",
    "    \n",
    "    # read back the output rules\n",
    "    outputs = open('seq_rule_output.txt', 'r').read().strip().split('\\n')\n",
    "    output_rules = []\n",
    "    for rule in outputs:\n",
    "        left, right, sup, conf = re.search(pattern=r'([0-9\\,]+) ==> ([0-9\\,]+) #SUP: ([0-9]+) #CONF: ([0-9\\.]+)', string=rule).groups()\n",
    "        sup = int(sup) / len(sequences)\n",
    "        conf = float(conf)\n",
    "        output_rules.append([[output_dict[x] for x in left.split(',')], [output_dict[x] for x in right.split(',')], sup, conf])\n",
    "    \n",
    "    # return pandas DataFrame\n",
    "    return pd.DataFrame(output_rules, columns = ['Left_rule', 'Right_rule', 'Support', 'Confidence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_association_rules(sequences, 0.055, 0.055)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Identify the top-5 common product categories that customers bought with the product category ‘01F’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LPF, IEV, N8U, OXH, FU5\n",
    "result_df_3[(result_df_3['Right_side'] == '01F') | (result_df_3['Left_side'] == '01F')].sort_values(by='Support', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LPF, IEV, N8U, OXH, FU5\n",
    "result_df_3[(result_df_3['Right_side'] == '01F') | (result_df_3['Left_side'] == '01F')].sort_values(by='Confidence', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "3bc1a11519d29b3ed6f08646f3ece60640217e649724c6bcfd38e1173c1a1bce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
